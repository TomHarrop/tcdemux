#!/usr/bin/env python3

from Bio import SeqIO
from Bio.Seq import Seq
from Bio.SeqRecord import SeqRecord
from pathlib import Path
from snakemake.logging import logger
import pandas as pd
import re
import shutil
import tempfile


#############
# FUNCTIONS #
#############


def get_dedupe_input(wildcards):
    if wildcards.readset == "before_processing":
        return get_repair_input(wildcards)
    elif wildcards.readset == "after_processing":
        return {
            "r1": Path(outdir, "{sample}.r1.fastq.gz"),
            "r2": Path(outdir, "{sample}.r2.fastq.gz"),
        }
    else:
        raise ValueError(f"Error parsing wildcards: {wildcards}")


def get_repair_input(wildcards):
    if samples_have_internal_barcodes:
        return {
            "r1": Path(workingdir, "020_demultiplex", "{sample}.r1.fastq"),
            "r2": Path(workingdir, "020_demultiplex", "{sample}.r2.fastq"),
        }
    else:
        return {
            "r1": Path(workingdir, "010_barcode-check", "{sample}.r1.fastq"),
            "r2": Path(workingdir, "010_barcode-check", "{sample}.r2.fastq"),
        }


def get_sample_barcode_seq(wildcards):
    sample = wildcards.sample
    my_sample_data = sample_data.loc[sample]
    i5_seq = my_sample_data["i5_index"]
    i7_seq = my_sample_data["i7_index"]
    i5_rc = Seq(i5_seq).reverse_complement()
    return f"{i7_seq}+{i5_rc}"


def get_sample_files(wildcards):
    sample = wildcards.sample
    my_sample_data = sample_data.loc[sample]
    r1_file = my_sample_data["r1_file"]
    r2_file = my_sample_data["r2_file"]
    return {
        "r1": Path(read_directory, r1_file),
        "r2": Path(read_directory, r2_file),
    }


# NOTE! Check how the external barcodes have been represented in the metadata.
# For example, in one set of metadata where the i5 index is listed as AGCGCTAG
# and the i7 index is listed as CCGCGGTT, the reads show up with headers like
# this: CCGCGGTT+CTAGCGCT. So the i5 index has to be reverse complemented.
def get_pool_barcode_seq(wildcards):
    pool = wildcards.pool
    pool_data = sample_data[sample_data["pool_name"] == pool]
    i5_seq = pool_data["i5_index"].iloc[0]
    i7_seq = pool_data["i7_index"].iloc[0]
    i5_rc = Seq(i5_seq).reverse_complement()
    return f"{i7_seq}+{i5_rc}"


def get_pool_files(wildcards):
    pool = wildcards.pool
    pool_data = sample_data[sample_data["pool_name"] == pool]
    r1_file = pool_data["r1_file"].iloc[0]
    r2_file = pool_data["r2_file"].iloc[0]
    return {
        "r1": Path(read_directory, r1_file),
        "r2": Path(read_directory, r2_file),
    }


def sample_name_sanitiser(sample_name):
    if re.compile("[^a-zA-Z0-9_-]").search(sample_name):
        raise ValueError(f"{sample_name} contains special character(s)")
    if "__" in sample_name:
        raise ValueError(f"{sample_name} contains double underscores")


# because demux is conditional this needs to be a function
def demux_stats_target(wildcards):
    targets = []
    # demuxing stats
    if samples_have_internal_barcodes:
        demux_stats = expand(
            Path(statdir, "demux", "{pool}.demux_stats.csv"), pool=all_pools
        )
        for x in demux_stats:
            targets.append(x)
    return targets


###########
# GLOBALS #
###########

# config from entrypoint
sample_data_file = Path(config["sample_data_file"])
read_directory = Path(config["read_directory"])

adaptor_files = config["adaptor_files"]

outdir = Path(config["outdir"])
logdir = Path(outdir, "logs")
statdir = Path(outdir, "stats")

max_threads = config["threads"]

keep_intermediate_files = config["keep_intermediate_files"]


########
# MAIN #
########

# set up working direcory
if keep_intermediate_files:
    logger.info(f"Keeping intermediate files in {outdir}")
    workingdir = Path(outdir, "intermediate_files")
else:
    workingdir = tempfile.mkdtemp()
    logger.info(f"Using {workingdir} for intermediate files")

# read the sample data and get a list of samples
sample_data = pd.read_csv(sample_data_file, index_col="name")

all_samples = sorted(set(sample_data.index))

# make sure there are no special characters
bad_samples = []
for sample in all_samples:
    try:
        sample_name_sanitiser(sample)
        logger.debug(f"sample {sample} appears sane")
    except ValueError as e:
        bad_samples.append(str(e))

if len(bad_samples) > 0:
    error_message = "Bad sample names found:\n" + "\n".join(bad_samples)
    raise ValueError(error_message)

# Check for the pool_name field.
# If the pool_name field is not present, we don't need to demultiplex internal
# barcodes, we only need to check the external barcodes for errors.
# This is done with with conditionals in the inputs
samples_have_internal_barcodes = False
all_pools = []
if "pool_name" in sample_data:
    samples_have_internal_barcodes = True
    all_pools = sorted(set(sample_data["pool_name"]))
    pool_threads = max(1, max_threads // len(all_pools))
    logger.info(f"Detected pool_name field in {sample_data_file}.")
    logger.info("The following pools will be demuxed using internal barcodes:")
    logger.info(f'{" ".join(all_pools)}')
    logger.info(f"Using {pool_threads} threads per pool for demuxing.")
else:
    logger.info(f"No pool_name field in {sample_data_file}.")
    logger.info("Not attempting to demux by internal barcode.")

# Configure log parsing. These fields will be grepped out of the bbduk logs and
# show up in the final CSVs.
log_fields = [
    "Containments:",
    "Duplicates Found:",
    "Duplicates:",
    "Entropy-masked:",
    "FTrimmed:",
    "Input:",
    "KTrimmed:",
    "Pairs:",
    "Reads In:",
    "Result:",
    "Singletons:",
]
log_regex = "\|".join(log_fields)

# These logs will be grepped for stats. Don't try to parse the demuxbyname
# logs, they have a different format.
log_steps = [
    "count_duplicates_after_processing",
    "count_duplicates_before_processing",
    "mask",
    "repair",
    "trim",
]


#########
# RULES #
#########


rule target:
    input:
        expand(
            Path(outdir, "{sample}.r{r}.fastq.gz"),
            sample=all_samples,
            r=[1, 2],
        ),
        expand(Path(outdir, "{sample}.unpaired.fastq.gz"), sample=all_samples),
        # stats steps
        demux_stats_target,
        expand(
            Path(logdir, "count_duplicates_{readset}", "{sample}.log"),
            sample=all_samples,
            readset=["before_processing", "after_processing"],
        ),
        expand(
            Path(statdir, "{step}.csv"),
            step=log_steps,
        ),
        Path(statdir, "adaptor_stats.pdf"),
        Path(statdir, "duplication_stats.pdf"),


##############
# PROCESSING #
##############


# mask low complexity regions
rule mask:
    input:
        pipe=Path(workingdir, "040_trim", "{sample}.paired.fastq"),
    output:
        r1=Path(outdir, "{sample}.r1.fastq.gz"),
        r2=Path(outdir, "{sample}.r2.fastq.gz"),
        stats=Path(statdir, "mask", "{sample}.txt"),
    log:
        Path(logdir, "mask", "{sample}.log"),
    threads: 1
    resources:
        time=lambda wildcards, attempt: 10 * attempt,
        mem_mb=lambda wildcards, attempt: 1e3 * attempt,
    shell:
        "bbduk.sh "
        "-Xmx{resources.mem_mb}m "
        "threads={threads} "
        "in={input.pipe} "
        "int=t "
        "out={output.r1} "
        "out2={output.r2} "
        "zl=9 "
        "entropy=0.8 "
        "entropywindow=20 "
        "entropymask=t "
        "stats={output.stats} "
        "2> {log} "


rule mask_unpaired:
    input:
        Path(workingdir, "040_trim", "{sample}.unpaired.fastq"),
    output:
        fq=Path(outdir, "{sample}.unpaired.fastq.gz"),
        stats=Path(statdir, "mask", "{sample}.unpaired.txt"),
    log:
        Path(logdir, "mask_unpaired", "{sample}.log"),
    threads: 1
    resources:
        time=lambda wildcards, attempt: 10 * attempt,
        mem_mb=lambda wildcards, attempt: 1e3 * attempt,
    shell:
        "bbduk.sh "
        "-Xmx{resources.mem_mb}m "
        "threads={threads} "
        "in={input} "
        "int=f "
        "out={output.fq} "
        "zl=9 "
        "entropy=0.8 "
        "entropywindow=20 "
        "entropymask=t "
        "stats={output.stats} "
        "2> {log} "


# trim adaptors
rule trim:
    input:
        pipe=Path(workingdir, "030_repair", "{sample}.fastq"),
        adaptors=Path(workingdir, "adaptors.fasta"),
    output:
        pipe=pipe(Path(workingdir, "040_trim", "{sample}.paired.fastq")),
        unpaired=pipe(Path(workingdir, "040_trim", "{sample}.unpaired.fastq")),
        stats=Path(statdir, "trim", "{sample}.txt"),
    params:
        ft=lambda wildcards: ""
        if samples_have_internal_barcodes
        else "forcetrimmod=5",
    log:
        Path(logdir, "trim", "{sample}.log"),
    threads: 2
    resources:
        time=lambda wildcards, attempt: 10 * attempt,
        mem_mb=lambda wildcards, attempt: 2e3 * attempt,
    shell:
        "bbduk.sh "
        "-Xmx{resources.mem_mb}m "
        "threads={threads} "
        "in={input.pipe} "
        "int=t "
        "out=stdout.fastq "
        "outs={output.unpaired} "
        "ref={input.adaptors} "
        "ktrim=r k=23 mink=11 hdist=1 tpe tbo "
        "{params.ft} "
        "stats={output.stats} "
        ">> {output.pipe} "
        "2> {log} "


# double check pairing
rule repair:
    input:
        unpack(get_repair_input),
    output:
        pipe(Path(workingdir, "030_repair", "{sample}.fastq")),
    log:
        Path(logdir, "repair", "{sample}.log"),
    threads: 1
    resources:
        time=lambda wildcards, attempt: 10 * attempt,
        mem_mb=lambda wildcards, attempt: 4e3 * attempt,
    shell:
        "repair.sh "
        "-Xmx{resources.mem_mb}m "
        "-Xms100m "
        "in={input.r1} "
        "in2={input.r2} "
        "out=stdout.fastq "
        "outs=/dev/null "
        "repair=t "
        "tossbrokenreads=t "
        "tossjunk=t "
        ">> {output} "
        "2> {log}"


rule combine_adaptors:
    input:
        adaptor_files,
    output:
        adaptors=Path(workingdir, "adaptors.fasta"),
        duplicates=Path(statdir, "duplicated_adaptors.fasta"),
    log:
        Path(logdir, "combine_adaptors.log"),
    threads: 1
    resources:
        time=lambda wildcards, attempt: 10 * attempt,
        mem_mb=4000,
    shell:
        "cat {input} | "
        "dedupe.sh "
        "-Xmx{resources.mem_mb}m "
        "in=stdin.fasta "
        "out={output.adaptors} "
        "outd={output.duplicates} "
        "absorbcontainment=f "
        "absorbrc=f "
        "ascending=t "
        "exact=t "
        "maxedits=0 "
        "maxsubs=0 "
        "sort=name "
        "touppercase=t "
        "uniquenames=t "
        "2> {log} "


##################
# DEMULTIPLEXING #
##################

# demux using cutadapt
# NOTE! I think the most straightforward way to do this is using cutadapt's
# [paired adaptor demultiplex mode]
# (https://cutadapt.readthedocs.io/en/stable/guide.html#unique-dual-indices)
# (so that both reads are checked). Alternatively, I could use cutadapt's
# [regular demultiplexing mode]
# (https://cutadapt.readthedocs.io/en/stable/guide.html#demultiplexing),
# although this would only check the first read:
# "Paired-end demultiplexing always uses the adapter matches of the first read to
#  decide where a read should be written. If adapters for read 2 are given
#  (-A/-G), they are detected and removed as normal, but these matches do not
#  influence where the read pair is written"
# Eventually it may be better to use multiple combinatorial adaptor mode to be
# safe.

for mypool in all_pools:
    pool_sd = sample_data[sample_data["pool_name"] == mypool]
    pool_samples = sorted(set(pool_sd.index))

    rule:
        input:
            Path(statdir, "demux", f"{mypool}.full_stats.json"),
        output:
            Path(statdir, "demux", f"{mypool}.demux_stats.csv"),
        shell:
            "parse_cutadapt_stats.py < {input} > {output}"

    rule:
        input:
            r1=Path(workingdir, "010_barcode-check", f"{mypool}.r1.fastq"),
            r2=Path(workingdir, "010_barcode-check", f"{mypool}.r2.fastq"),
            barcodes=Path(
                workingdir, "020_demultiplex", f"{mypool}.barcodes.fasta"
            ),
        output:
            r1=expand(
                Path(workingdir, "020_demultiplex", "{sample}.r1.fastq"),
                sample=pool_samples,
            ),
            r2=expand(
                Path(workingdir, "020_demultiplex", "{sample}.r2.fastq"),
                sample=pool_samples,
            ),
            stats=Path(statdir, "demux", f"{mypool}.full_stats.json"),
        params:
            outdir=lambda wildcards, output: Path(output[0]).parent.as_posix(),
        log:
            Path(logdir, "demux", f"{mypool}.log"),
        threads: max(2, pool_threads)
        shell:
            "cutadapt "
            "-j {threads} "
            "-e 0 "
            "--no-indels "
            "--pair-adapters "
            "-g ^file:{input.barcodes} "
            "-G ^file:{input.barcodes} "
            "-o {params.outdir}/{{name}}.r1.fastq "
            "-p {params.outdir}/{{name}}.r2.fastq "
            "--json {output.stats} "
            "{input.r1} "
            "{input.r2} "
            "&> {log}"

    rule:
        input:
            sample_data=sample_data_file,
        output:
            barcode_file=Path(
                workingdir, "020_demultiplex", f"{mypool}.barcodes.fasta"
            ),
        params:
            pool=mypool,
        script:
            shutil.which("write_barcode_file.py")


# check barcodes in pooled fastq files
with tempfile.TemporaryDirectory() as rule_tmpdir:

    rule check_pool_barcodes:
        input:
            unpack(get_pool_files),
        output:
            r1=Path(workingdir, "010_barcode-check", "{pool}.r1.fastq"),
            r2=Path(workingdir, "010_barcode-check", "{pool}.r2.fastq"),
            stats=Path(statdir, "check_barcodes", "{pool}.txt"),
        params:
            barcode_seq=lambda wildcards: get_pool_barcode_seq(wildcards),
            out=Path(rule_tmpdir, "%.r1.fastq").as_posix(),
            out2=Path(rule_tmpdir, "%.r2.fastq").as_posix(),
            streams=2,
        log:
            Path(logdir, "check_pool_barcodes", "{pool}.log"),
        threads: 2
        resources:
            mem_mb=8000,
        shell:
            "mkdir -p {rule_tmpdir} ; "
            "demuxbyname.sh "
            "delimiter=: prefixmode=f "
            "names={params.barcode_seq} "
            "out={params.out} "
            "out2={params.out2} "
            "outu=/dev/null "
            "stats={output.stats} "
            "in={input.r1} "
            "in2={input.r2} "
            "streams={params.streams} "
            "threads={threads} "
            "-Xmx{resources.mem_mb}m "
            "-Xms100m "
            "2> {log} "
            "&& "
            "mv {rule_tmpdir}/{params.barcode_seq}.r1.fastq "
            "{output.r1} "
            "&& "
            "mv {rule_tmpdir}/{params.barcode_seq}.r2.fastq "
            "{output.r2}"


# check barcodes in individual barcode files
with tempfile.TemporaryDirectory() as tmpdir:

    rule check_sample_barcodes:
        input:
            unpack(get_sample_files),
        output:
            r1=Path(workingdir, "010_barcode-check", "{sample}.r1.fastq"),
            r2=Path(workingdir, "010_barcode-check", "{sample}.r2.fastq"),
            stats=Path(statdir, "check_barcodes", "{sample}.txt"),
        params:
            barcode_seq=lambda wildcards: get_sample_barcode_seq(wildcards),
            out=Path(tmpdir, "%.r1.fastq").as_posix(),
            out2=Path(tmpdir, "%.r2.fastq").as_posix(),
            streams=2,
        log:
            Path(logdir, "check_sample_barcodes", "{sample}.log"),
        threads: 2
        resources:
            mem_mb=8000,
        shell:
            "demuxbyname.sh "
            "delimiter=: prefixmode=f "
            "names={params.barcode_seq} "
            "out={params.out} "
            "out2={params.out2} "
            "outu=/dev/null "
            "stats={output.stats} "
            "in={input.r1} "
            "in2={input.r2} "
            "streams={params.streams} "
            "threads={threads} "
            "-Xmx{resources.mem_mb}m "
            "-Xms100m "
            "2> {log} "
            "&& "
            "mv {tmpdir}/{params.barcode_seq}.r1.fastq "
            "{output.r1} "
            "&& "
            "mv {tmpdir}/{params.barcode_seq}.r2.fastq "
            "{output.r2}"


#########
# STATS #
#########


rule plot_adaptor_stats:
    input:
        trim_files=expand(
            Path(statdir, "trim", "{sample}.txt"), sample=all_samples
        ),
        summary_file=Path(statdir, "trim.csv"),
    output:
        plot=Path(statdir, "adaptor_stats.pdf"),
    log:
        Path(logdir, "plot_adaptor_stats.log"),
    script:
        shutil.which("plot_adaptor_stats.R")


rule plot_duplication_stats:
    input:
        before_file=Path(statdir, "count_duplicates_before_processing.csv"),
        after_file=Path(statdir, "count_duplicates_after_processing.csv"),
    output:
        plot=Path(statdir, "duplication_stats.pdf"),
    log:
        Path(logdir, "plot_duplication_stats.log"),
    script:
        shutil.which("plot_duplication_stats.R")


rule combine_step_logs:
    input:
        expand(
            Path(workingdir, "from_logs", "{{step}}", "{sample}.csv"),
            sample=all_samples,
        ),
    output:
        Path(statdir, "{step}.csv"),
    shell:
        "echo 'sample,type,reads,bases' > {output} ; "
        "cat {input} >> {output}"


rule process_step_logs:
    input:
        Path(workingdir, "from_logs", "{step}", "{sample}.txt"),
    output:
        temp(Path(workingdir, "from_logs", "{step}", "{sample}.csv")),
    shell:
        "process_step_logs.py {wildcards.sample} < {input} > {output} "


rule grep_logs:
    input:
        Path(logdir, "{step}", "{sample}.log"),
    output:
        temp(Path(workingdir, "from_logs", "{step}", "{sample}.txt")),
    shell:
        # awk -F '[[:space:]]{2,}' means separated by two or more spaces
        "grep '^\({log_regex}\)' {input} "
        "| "
        "awk -F '[[:space:]]{{2,}}' "
        "'{{print $1, $2, $3}}' "
        "OFS='\t' "
        "> {output} "


rule count_duplicates:
    input:
        unpack(get_dedupe_input),
    log:
        Path(logdir, "count_duplicates_{readset}", "{sample}.log"),
    threads: lambda wildcards, attempt: int(2**attempt)
    resources:
        time=lambda wildcards, attempt: 10 * attempt,
        mem_mb=lambda wildcards, attempt: (4 * (2**attempt)) * 1e3,
    shell:
        "timeout {resources.time}m "
        "clumpify.sh "
        "-Xmx{resources.mem_mb}m "
        "-Xms100m "
        "threads={threads} "
        "in={input.r1} "
        "in2={input.r2} "
        "dedupe=t "
        "subs=0 "
        "usetmpdir=t "
        "2> {log}"
